
* Change cache directory
https://huggingface.co/docs/transformers/installation#cache-setup
https://stackoverflow.com/questions/61798573/where-does-hugging-faces-transformers-save-models

The default path of cache directory is
- "~/.cache/huggingface/hub" as of transformers 4.22
- "~/.cache/torch/transformers" otherwise

If you want to change it, specify the path with =HUGGINGFACE_HUB_CACHE= or =TRANSFORMERS_CACHE=.

e.g. Running python code with a specified cache location
#+begin_src sh
# absolute path
HUGGINGFACE_HUB_CACHE=./some/path python code.py
# TRANSFORMERS_CACHE=./some/path python code.py

# path with user's home
HUGGINGFACE_HUB_CACHE=~/some/path python code.py
# TRANSFORMERS_CACHE=~/some/path python code.py

# export the path
export HUGGINGFACE_HUB_CACHE=some/path
python code.py
#+end_src

* Downloading a model
https://huggingface.co/docs/hub/models-downloading

#+begin_src sh
git lfs install

MODEL_NAME=facebook/bart-base
MODEL_URL=https://huggingface.co/$MODEL_NAME
git clone git clone $MODEL_URL
#+end_src

* Auto classes
https://huggingface.co/docs/transformers/model_doc/auto#auto-classes
https://huggingface.co/docs/transformers/autoclass_tutorial
- AutoTokenizer
- AutoModel
  - AutoModelForSeq2SeqLM
  - AutoModelForSequenceClassification
  - etc
- AutoConfig
- etc
** AutoProcessor
https://huggingface.co/docs/transformers/preprocessing

- AutoProcessor is more general than AutoTokenizer or AutoFeatureExtractor
- AutoProcessor also can be used to preprocess multi-modal data
- e.g.
  #+begin_src python
  type(AutoProcessor.from_pretrained("bert-base-cased")) is type(AutoTokenizer.from_pretrained("bert-base-cased"))
  #+end_src

* Examples
** running with scripts
https://huggingface.co/docs/transformers/run_scripts
https://github.com/huggingface/transformers/tree/main/examples/pytorch
** run a model after a tokenizer
#+begin_src python
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
pt_outputs = pt_model(**pt_batch)
#+end_src
** run a pipeline with a model and a tokenizer
#+begin_src python
from transformers import pipeline
generator = pipeline(task="text-generation", model=model, tokenizer=tokenizer)
#+end_src
** tokenize and decode
#+begin_src python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
tokenizer.decode(encoded_input["input_ids"])
#+end_src
** tokenizer example
https://huggingface.co/transformers/v3.0.2/preprocessing.html
** PreTrainedTokenizer.convert_ids_to_tokens
it maps token ids to tokens
