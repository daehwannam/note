
* Change cache directory
https://huggingface.co/docs/transformers/installation#cache-setup
https://stackoverflow.com/questions/61798573/where-does-hugging-faces-transformers-save-models

The default path of cache directory is
- "~/.cache/huggingface/hub" as of transformers 4.22
- "~/.cache/torch/transformers" otherwise

If you want to change it, specify the path with =HUGGINGFACE_HUB_CACHE= or =TRANSFORMERS_CACHE=.

e.g. Running python code with a specified cache location
#+begin_src sh
# absolute path
HUGGINGFACE_HUB_CACHE=./some/path python code.py
# TRANSFORMERS_CACHE=./some/path python code.py

# path with user's home
HUGGINGFACE_HUB_CACHE=~/some/path python code.py
# TRANSFORMERS_CACHE=~/some/path python code.py

# export the path
export HUGGINGFACE_HUB_CACHE=some/path
python code.py
#+end_src

* Downloading a model
https://huggingface.co/docs/hub/models-downloading

#+begin_src sh
git lfs install

MODEL_NAME=facebook/bart-base
MODEL_URL=https://huggingface.co/$MODEL_NAME
git clone git clone $MODEL_URL
#+end_src

* Auto classes
https://huggingface.co/docs/transformers/model_doc/auto#auto-classes
https://huggingface.co/docs/transformers/autoclass_tutorial
- AutoTokenizer
- AutoModel
  - AutoModelForSeq2SeqLM
  - AutoModelForSequenceClassification
  - etc
- AutoConfig
- etc
** AutoProcessor
https://huggingface.co/docs/transformers/preprocessing

- AutoProcessor is more general than AutoTokenizer or AutoFeatureExtractor
- AutoProcessor also can be used to preprocess multi-modal data
- e.g.
  #+begin_src python
  type(AutoProcessor.from_pretrained("bert-base-cased")) is type(AutoTokenizer.from_pretrained("bert-base-cased"))
  #+end_src

* Tokenizer
** tokenize and decode
#+begin_src python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokens = tokenizer.tokenize("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
encoded = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
tokenizer.decode(encoded["input_ids"])
#+end_src
*** decoding without special tokens
Use ~skip_special_tokens~ option
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
encoded = tokenizer("Hello World")
# {'input_ids': [0, 725, 33796, 623, 2], 'attention_mask': [1, 1, 1, 1, 1]}
print(tokenizer.decode(encoded['input_ids']))
# '<s>Hello World</s>'
print(tokenizer.decode(encoded['input_ids'], skip_special_tokens=True))
# 'Hello World'
#+end_src
** "add_prefix_space" options
RobertaTokenizer (and its subclass BartTokenizer) has ~add_prefix_space~ parameter for ~__init__~ and ~__call__~.
The option add an additional space character to the beginning of the input sentence.

- Example 1
  #+begin_src python
  from transformers import RobertaTokenizer
  tokenizer1 = RobertaTokenizer.from_pretrained("roberta-base")
  print(tokenizer1("Hello world"))
  # {'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  print(tokenizer1("Hello world", add_prefix_space=True))
  # {'input_ids': [0, 20920, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  print(tokenizer1(" Hello world")) # input text includes a space character at the beginning
  # {'input_ids': [0, 20920, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  #+end_src
- Example 2
  #+begin_src python
  from transformers import RobertaTokenizer
  tokenizer2 = RobertaTokenizer.from_pretrained("roberta-base", add_prefix_space=True)
  print(tokenizer2("Hello world"))
  # {'input_ids': [0, 20920, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  print(tokenizer2("Hello world", add_prefix_space=False))
  # {'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  #+end_src
** tensors as results
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
encoded = tokenizer(['Hello World!', 'My mother likes flowers.'], padding=True, return_tensors="pt")
#+end_src
** PreTrainedTokenizer.convert_ids_to_tokens
it maps token ids to tokens
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
encoded = tokenizer('Hello World!')
print(tokenizer.convert_ids_to_tokens(encoded['input_ids']))
# ['<s>', 'Hello', 'Ä World', '!', '</s>']
#+end_src
** convert functions
#+begin_src python
tokenizer.convert_ids_to_tokens([1,2])
# ['<pad>', '</s>']
tokenizer.convert_ids_to_tokens([1,2,-1])
# ['<pad>', '</s>', None]
tokenizer.convert_ids_to_tokens([1,2,100000000000000])
# ['<pad>', '</s>', None]
tokenizer.convert_ids_to_tokens([1,2])
# ['<pad>', '</s>']
tokenizer.convert_ids_to_tokens(1)
# '<pad>'
tokenizer.convert_tokens_to_ids(['<pad>', '</s>'])
# [1, 2]
tokenizer.convert_tokens_to_ids('<pad>')
# 1
tokenizer.convert_tokens_to_string(['<pad>', '</s>'])
# '<pad></s>'
#+end_src
** batch-decoding
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")

encoded = tokenizer(['I like an apple.', 'A bird flies.'])
# {'input_ids': [[0, 100, 101, 41, 15162, 4, 2], [0, 250, 5103, 16016, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}
utteances = tokenizer.batch_decode(encoded['input_ids'], skip_special_tokens=True)
# ['I like an apple.', 'A bird flies.']
#+end_src
** tokenizer example
https://huggingface.co/transformers/v3.0.2/preprocessing.html
** iterate tokens
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
all_token_values = tuple(map(tokenizer.convert_ids_to_tokens, range(tokenizer.vocab_size)))
#+end_src
*** iterate non-special tokens
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
all_special_ids = set(tokenizer.all_special_ids)
all_non_special_tokens = tuple(
    tokenizer.convert_ids_to_tokens(token_id)
    for token_id in range(tokenizer.vocab_size)
    if token_id not in all_special_ids)
return all_non_special_tokens
#+end_src
** GPT2Tokenizer
- BartTokenizer is a subclass of GPT2Tokenizer
*** ~_convert_id_to_token~
~GPT2Tokenizer~ has has attributes named as ~encoder~ and ~decoder~ which are dictionaries
that map tokens to ids and ids to tokens respectively.
~GPT2Tokenizer._convert_id_to_token~ exploits ~encoder~ and ~decoder~.

* PreTrainedModel
** resize_token_embeddings
e.g.
#+begin_src python
from transformers import BartTokenizer

tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
new_tokens = ['<special-1>', '<special-2>']
added_tokens_num = tokenizer.add_tokens(new_tokens, special_tokens=True)
model.resize_token_embeddings(len(tokenizer))
#+end_src

* Examples
** running with scripts
https://huggingface.co/docs/transformers/run_scripts
https://github.com/huggingface/transformers/tree/main/examples/pytorch
** run a model after a tokenizer
#+begin_src python
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
pt_outputs = pt_model(**pt_batch)
#+end_src
** run a pipeline with a model and a tokenizer
#+begin_src python
from transformers import pipeline
generator = pipeline(task="text-generation", model=model, tokenizer=tokenizer)
#+end_src

* Accelerate
** Install
#+begin_src sh
pip install accelerate
#+end_src

** Make a config file
#+begin_src sh
CONFIG_FILE=path/to/config.yaml
mkdir -p $(dirname $CONFIG_FILE)
accelerate config --config_file $CONFIG_FILE
#+end_src

** Accelerator
*** ~__init__~
**** split_batches (default = False)
- When it's True, the size of batch of a data loader is divided by the number of processes.
  Therefore, the number of examples in batches in all processes is the batch size of the data loader.
  (In other words, The batch size of each process is 1/#processes of the batch size of the data loader.)
- When it's False, the batch of each process has the same batch size of the data loader.
**** even_batches (default = True)
- When it's True, each process has exactly the same number of batches for each epoch.
  To make the even number of batches, some processes have batches that include examples that are already used in some processes.
- When it's False, each process can have different numbers of batches.
  The difference the numbers of batches between two processes is up to 1 batch.
  However, the difference can raises a deadlock condition when computing Accelerator.backward.

** split_between_processes
The function "split_between_processes" is included in Accelerator, AcceleratorState or PartialState

#+begin_src python
# Usage:
# $ accelerate launch --num_processes 2 example.py
#
# Prerequisites:
# - To use N processes, CUDA_VISIBLE_DEVICES should contain N GPU numbers

# Assume there are two processes
from accelerate import PartialState

state = PartialState()
with state.split_between_processes(["A", "B", "C"]) as inputs:
    print(inputs)

# # Process 0
# ["A", "B"]
# # Process 1
# ["C"]

with state.split_between_processes(["D", "E", "F"], apply_padding=True) as inputs:
    print(inputs)

# # Process 0
# ["D", "E"]
# # Process 1
# ["F", "F"]
#+end_src

** AcceleratedOptimizer
by default, AcceleratedOptimizer adds the number of processes to self.optimizer.last_epoch for each synchronized epoch.

** Examples
*** Accelerator.backward
Accelerator.backward assumes that loss values on a batch is averaged rather than summed.

#+begin_src python
# Usage:
# CUDA_VISIBLE_DEVICES=0,1 accelerate launch --num_processes 2 accelerate_backward.py

from accelerate import Accelerator
import torch


accelerator = Accelerator(split_batches=True, even_batches=False)

class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.params = torch.nn.Parameter(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))

    def forward(self, batch):
        # Note: compute the mean of loss values from examples rather than sum of the values

        # loss = (self.params * batch).sum(dim=1).sum(dim=0)
        loss = (self.params * batch).sum(dim=1).mean()
        return loss


model = Model()

tensor = torch.tensor([2, 2, 2, 2, 2], dtype=torch.float)
data_loader = torch.utils.data.DataLoader([tensor] * 20, batch_size=4, shuffle=False)
optimizer = torch.optim.SGD(model.parameters(), lr=1.0)

model, data_loader, optimizer = accelerator.prepare(model, data_loader, optimizer)

for batch in data_loader:
    loss = model(batch)
    print('Batch-size: ', batch.shape[0])
    print('Loss: ', loss)

    optimizer.zero_grad()
    accelerator.backward(loss)
    # loss.backward()

    print('Gradients: ', tuple(model.parameters())[0].grad)

    optimizer.step()
    print('Parameters: ', tuple(model.parameters()))

    break
#+end_src
