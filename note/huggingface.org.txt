
* Change cache directory
https://huggingface.co/docs/transformers/installation#cache-setup

#+begin_src sh
# Example 1
TRANSFORMERS_CACHE=./some/path python code.py

# Example 3
TRANSFORMERS_CACHE=~/some/path python code.py
#+end_src

* Auto classes
https://huggingface.co/docs/transformers/model_doc/auto#auto-classes
- AutoTokenizer
- AutoModel
  - AutoModelForSeq2SeqLM
  - AutoModelForSequenceClassification
  - etc
- AutoConfig

* Examples
** run a model after a tokenizer
#+begin_src python
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
pt_outputs = pt_model(**pt_batch)
#+end_src
** run a pipeline with a model and a tokenizer
#+begin_src python
from transformers import pipeline
generator = pipeline(task="text-generation", model=model, tokenizer=tokenizer)
#+end_src
