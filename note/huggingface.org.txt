
* Change cache directory
https://huggingface.co/docs/transformers/installation#cache-setup
https://stackoverflow.com/questions/61798573/where-does-hugging-faces-transformers-save-models

The default path of cache directory is
- "~/.cache/huggingface/hub" as of transformers 4.22
- "~/.cache/torch/transformers" otherwise

If you want to change it, specify the path with =HUGGINGFACE_HUB_CACHE= or =TRANSFORMERS_CACHE=.

e.g. Running python code with a specified cache location
#+begin_src sh
# absolute path
HUGGINGFACE_HUB_CACHE=./some/path python code.py
# TRANSFORMERS_CACHE=./some/path python code.py

# path with user's home
HUGGINGFACE_HUB_CACHE=~/some/path python code.py
# TRANSFORMERS_CACHE=~/some/path python code.py

# export the path
export HUGGINGFACE_HUB_CACHE=some/path
python code.py
#+end_src

* Downloading a model
https://huggingface.co/docs/hub/models-downloading

#+begin_src sh
git lfs install

MODEL_NAME=facebook/bart-base
MODEL_URL=https://huggingface.co/$MODEL_NAME
git clone git clone $MODEL_URL
#+end_src

* Auto classes
https://huggingface.co/docs/transformers/model_doc/auto#auto-classes
https://huggingface.co/docs/transformers/autoclass_tutorial
- AutoTokenizer
- AutoModel
  - AutoModelForSeq2SeqLM
  - AutoModelForSequenceClassification
  - etc
- AutoConfig
- etc
** AutoProcessor
https://huggingface.co/docs/transformers/preprocessing

- AutoProcessor is more general than AutoTokenizer or AutoFeatureExtractor
- AutoProcessor also can be used to preprocess multi-modal data
- e.g.
  #+begin_src python
  type(AutoProcessor.from_pretrained("bert-base-cased")) is type(AutoTokenizer.from_pretrained("bert-base-cased"))
  #+end_src

* Tokenizer
** tokenize and decode
#+begin_src python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
tokenizer.decode(encoded_input["input_ids"])
#+end_src
** "add_prefix_space" options
RobertaTokenizer (and its subclass BartTokenizer) has ~add_prefix_space~ parameter for ~__init__~ and ~__call__~.
The option add an additional space character to the beginning of the input sentence.

- Example 1
  #+begin_src python
  from transformers import RobertaTokenizer
  tokenizer1 = RobertaTokenizer.from_pretrained("roberta-base")
  print(tokenizer1("Hello world"))
  # {'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  print(tokenizer1("Hello world", add_prefix_space=True))
  # {'input_ids': [0, 20920, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  print(tokenizer1(" Hello world"))
  # {'input_ids': [0, 20920, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  #+end_src
- Example 2
  #+begin_src python
  from transformers import RobertaTokenizer
  tokenizer2 = RobertaTokenizer.from_pretrained("roberta-base", add_prefix_space=True)
  print(tokenizer2("Hello world"))
  # {'input_ids': [0, 20920, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  print(tokenizer2("Hello world", add_prefix_space=False))
  # {'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  #+end_src
** PreTrainedTokenizer.convert_ids_to_tokens
it maps token ids to tokens
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
encoded_input = tokenizer('Hello World!')
print(tokenizer.convert_ids_to_tokens(encoded_input['input_ids']))
# ['<s>', 'Hello', 'Ä World', '!', '</s>']
#+end_src
** tokenizer example
https://huggingface.co/transformers/v3.0.2/preprocessing.html

* Examples
** running with scripts
https://huggingface.co/docs/transformers/run_scripts
https://github.com/huggingface/transformers/tree/main/examples/pytorch
** run a model after a tokenizer
#+begin_src python
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
pt_outputs = pt_model(**pt_batch)
#+end_src
** run a pipeline with a model and a tokenizer
#+begin_src python
from transformers import pipeline
generator = pipeline(task="text-generation", model=model, tokenizer=tokenizer)
#+end_src
