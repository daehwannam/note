
* Change cache directory
https://huggingface.co/docs/transformers/installation#cache-setup
https://stackoverflow.com/questions/61798573/where-does-hugging-faces-transformers-save-models

The default path of cache directory is
- "~/.cache/huggingface/hub" as of transformers 4.22
- "~/.cache/torch/transformers" otherwise

If you want to change it, specify the path with =HUGGINGFACE_HUB_CACHE= or =TRANSFORMERS_CACHE=.

e.g. Running python code with a specified cache location
#+begin_src sh
# absolute path
HUGGINGFACE_HUB_CACHE=./some/path python code.py
# TRANSFORMERS_CACHE=./some/path python code.py

# path with user's home
HUGGINGFACE_HUB_CACHE=~/some/path python code.py
# TRANSFORMERS_CACHE=~/some/path python code.py

# export the path
export HUGGINGFACE_HUB_CACHE=some/path
python code.py
#+end_src

* Downloading a model
https://huggingface.co/docs/hub/models-downloading

#+begin_src sh
git lfs install

MODEL_NAME=facebook/bart-base
MODEL_URL=https://huggingface.co/$MODEL_NAME
git clone git clone $MODEL_URL
#+end_src

* Auto classes
https://huggingface.co/docs/transformers/model_doc/auto#auto-classes
https://huggingface.co/docs/transformers/autoclass_tutorial
- AutoTokenizer
- AutoModel
  - AutoModelForSeq2SeqLM
  - AutoModelForSequenceClassification
  - etc
- AutoConfig
- etc
** AutoProcessor
https://huggingface.co/docs/transformers/preprocessing

- AutoProcessor is more general than AutoTokenizer or AutoFeatureExtractor
- AutoProcessor also can be used to preprocess multi-modal data
- e.g.
  #+begin_src python
  type(AutoProcessor.from_pretrained("bert-base-cased")) is type(AutoTokenizer.from_pretrained("bert-base-cased"))
  #+end_src

* Tokenizer
** tokenize and decode
#+begin_src python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokens = tokenizer.tokenize("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
encoded = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
tokenizer.decode(encoded["input_ids"])
#+end_src
*** decoding without special tokens
Use ~skip_special_tokens~ option
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
encoded = tokenizer("Hello World")
# {'input_ids': [0, 725, 33796, 623, 2], 'attention_mask': [1, 1, 1, 1, 1]}
print(tokenizer.decode(encoded['input_ids']))
# '<s>Hello World</s>'
print(tokenizer.decode(encoded['input_ids'], skip_special_tokens=True))
# 'Hello World'
#+end_src
** "add_prefix_space" options
RobertaTokenizer (and its subclass BartTokenizer) has ~add_prefix_space~ parameter for ~__init__~ and ~__call__~.
The option add an additional space character to the beginning of the input sentence.

- Example 1
  #+begin_src python
  from transformers import RobertaTokenizer
  tokenizer1 = RobertaTokenizer.from_pretrained("roberta-base")
  print(tokenizer1("Hello world"))
  # {'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  print(tokenizer1("Hello world", add_prefix_space=True))
  # {'input_ids': [0, 20920, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  print(tokenizer1(" Hello world")) # input text includes a space character at the beginning
  # {'input_ids': [0, 20920, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  #+end_src
- Example 2
  #+begin_src python
  from transformers import RobertaTokenizer
  tokenizer2 = RobertaTokenizer.from_pretrained("roberta-base", add_prefix_space=True)
  print(tokenizer2("Hello world"))
  # {'input_ids': [0, 20920, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  print(tokenizer2("Hello world", add_prefix_space=False))
  # {'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  #+end_src
** PreTrainedTokenizer.convert_ids_to_tokens
it maps token ids to tokens
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
encoded = tokenizer('Hello World!')
print(tokenizer.convert_ids_to_tokens(encoded['input_ids']))
# ['<s>', 'Hello', 'Ä World', '!', '</s>']
#+end_src
** convert functions
#+begin_src python
tokenizer.convert_ids_to_tokens([1,2])
# ['<pad>', '</s>']
tokenizer.convert_ids_to_tokens([1,2,-1])
# ['<pad>', '</s>', None]
tokenizer.convert_ids_to_tokens([1,2,100000000000000])
# ['<pad>', '</s>', None]
tokenizer.convert_ids_to_tokens([1,2])
# ['<pad>', '</s>']
tokenizer.convert_ids_to_tokens(1)
# '<pad>'
tokenizer.convert_tokens_to_ids(['<pad>', '</s>'])
# [1, 2]
tokenizer.convert_tokens_to_ids('<pad>')
# 1
tokenizer.convert_tokens_to_string(['<pad>', '</s>'])
# '<pad></s>'
#+end_src
** batch-decoding
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")

encoded = tokenizer(['I like an apple.', 'A bird flies.'])
# {'input_ids': [[0, 100, 101, 41, 15162, 4, 2], [0, 250, 5103, 16016, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}
utteances = tokenizer.batch_decode(encoded['input_ids'], skip_special_tokens=True)
# ['I like an apple.', 'A bird flies.']
#+end_src
** tokenizer example
https://huggingface.co/transformers/v3.0.2/preprocessing.html
** iterate tokens
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
all_token_values = tuple(map(tokenizer.convert_ids_to_tokens, range(tokenizer.vocab_size)))
#+end_src
*** iterate non-special tokens
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
all_special_ids = set(tokenizer.all_special_ids)
all_non_special_tokens = tuple(
    tokenizer.convert_ids_to_tokens(token_id)
    for token_id in range(tokenizer.vocab_size)
    if token_id not in all_special_ids)
return all_non_special_tokens
#+end_src
** GPT2Tokenizer
- BartTokenizer is a subclass of GPT2Tokenizer
*** ~_convert_id_to_token~
~GPT2Tokenizer~ has has attributes named as ~encoder~ and ~decoder~ which are dictionaries
that map tokens to ids and ids to tokens respectively.
~GPT2Tokenizer._convert_id_to_token~ exploits ~encoder~ and ~decoder~.

* PreTrainedModel
** resize_token_embeddings
e.g.
#+begin_src python
from transformers import BartTokenizer

tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
new_tokens = ['<special-1>', '<special-2>']
added_tokens_num = tokenizer.add_tokens(new_tokens, special_tokens=True)
model.resize_token_embeddings(len(tokenizer))
#+end_src

* Examples
** running with scripts
https://huggingface.co/docs/transformers/run_scripts
https://github.com/huggingface/transformers/tree/main/examples/pytorch
** run a model after a tokenizer
#+begin_src python
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
pt_outputs = pt_model(**pt_batch)
#+end_src
** run a pipeline with a model and a tokenizer
#+begin_src python
from transformers import pipeline
generator = pipeline(task="text-generation", model=model, tokenizer=tokenizer)
#+end_src
