
* Note
- "conda install" command doesn't work on conda environments

* Install
pip3 install torch torchvision

* expand vs. repeat
https://guide.allennlp.org/document-ranking#4

both .expand and .repeat can change the size of dimensions.
but theres some differences:
- .expand cannot change the number of dimensions
- .expand doesn't copy memory

* examples of pack_padded_sequence and pad_packed_sequence
- https://gist.github.com/Tushar-N/dfca335e370a2bc3bc79876e6270099e
- https://gist.github.com/MikulasZelinka/9fce4ed47ae74fca454e88a39f8d911a

* Structure of last_hidden & last_cell of LSTM
https://pytorch.org/docs/1.8.0/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM
https://pytorch.org/docs/1.8.0/generated/torch.nn.RNN.html?highlight=rnn#torch.nn.RNN

The returned value ~h_n~ and ~c_n~ of LSTM has the shape of ~(num_layers * num_directions, batch, hidden_size)~.
It can be reshaped as ~h_n.view(num_layers, num_directions, batch, hidden_size)~.

#+begin_src python
import torch

seq_len = 5
batch_size = 3
input_size = 10

hidden_size = 20
num_layers = 2
dropout = 0.3
bidirectional = True
num_directions = 2 if bidirectional else 1

rnn = torch.nn.LSTM(input_size, hidden_size, num_layers, bidirectional=bidirectional)

input = torch.randn(seq_len, batch_size, input_size)
h0 = torch.randn(num_layers * num_directions, batch_size, hidden_size)
c0 = torch.randn(num_layers * num_directions, batch_size, hidden_size)
output, (hn, cn) = rnn(input, (h0, c0))

assert hn.shape == cn.shape == torch.Size([num_layers * num_directions, batch_size, hidden_size])
reshaped_hn = hn.view(num_layers, num_directions, batch_size, hidden_size)
#+end_src

* DataLoader collate_fn
https://discuss.pytorch.org/t/how-to-use-collate-fn/27181

* ~lr_scheduler~
- when an ~_LRScheduler~ instance is created, ~__init__~ calls ~self.step~
- ~self.step~ executes ~self.last_epoch += 1~ every time
- the default value of ~last_epoch~ of ~__init__~ is -1
- when -1 is passed as ~last_epoch~, ~self.last_epoch~ becomes -1 and changes to 0 as ~self.step~ is executed
- you can call ~lr_scheduler.step~ not only for epoch but also for every step
- usually, ~optimizer.step~ is called first, then ~lr_scheduler.step~ is called
- ~LambdaLR.__init__~ requires a function named ~lr_lambda~ that takes ~LambdaLR.last_epoch~

* tokenizer
** clean_up_tokenization_spaces of PreTrainedTokenizerBase.decode
~clean_up_tokenization_spaces~, an argument of ~PreTrainedTokenizerBase.decode~, modifies output text.
~decode~ exploits ~PreTrainedTokenizerBase.clean_up_tokenization~.
