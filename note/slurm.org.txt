
* Usage
** sinfo
Check recource

[Command]
#+begin_src sh
sinfo
#+end_src

[Output]
PARTITION  CPUS(A/I/O/T)  STATE  NODELIST  NODES GRES             MEMORY  TIMELIMIT   AVAIL_FEATURES
2080ti*    4/76/0/80      mix    n[1-4]    4     gpu:GTX2080Ti:8  120000  3-00:00:00  (null)        
titanrtx   2/38/0/40      mix    n[5,7]    2     gpu:TITANRTX:4   250000  3-00:00:00  (null)        
titanrtx   0/88/0/88      idle   n[6,8-9]  3     gpu:TITANRTX:4   250000  3-00:00:00  (null)        
titanxp    16/24/0/40     mix    n[10-11]  2     gpu:TITANXP:8    120000  3-00:00:00  (null)        
titanxp    0/40/0/40      idle   n[12-13]  2     gpu:TITANXP:8    120000  3-00:00:00  (null)        

[Details]
- There exist 5 partitions, where each partition has several nodes
- Information of CPUS indicates Allocated/Idle/Other/Total (A/I/O/T)
- TIMELIMIT indicate the maximum allowed time for each job (e.g. 3 days)

** squeue
Check slurm queue

[Commands]
#+begin_src sh
squeue
squeue -u $USER
#+end_src

[Output]
JOBID  NAME          STATE     USER     GROUP    PARTITION  NODE NODELIST(REASON)  CPUS TRES_PER_NODE TIME_LIMIT  TIME_LEFT  
51617  train-8x2080  RUNNING   aren     usercl4  2080ti     1    n2                1    gpu:8         3-00:00:00  2-07:15:18 
51575  0.5.4.2.7p4   RUNNING   weebum   usercl4  titanxp    1    n11               2    gpu:1         2-23:00:00  1-00:50:06 
51556  unfreeze      RUNNING   lee1jun  usercl4  titanxp    1    n10               8    gpu:8         3-00:00:00  7:42:58

[Details]
- State: running/pendding/cg
  https://ubccr.freshdesk.com/support/solutions/articles/5000686861-how-do-i-check-the-status-of-my-job-s-
  - running: job is running
  - pendding: job is waiting to run
  - cg: The job is done. It disappears soon

- Nodelist (reason)
  https://slurm.schedmd.com/quickstart.html
  - recources: waiting for resources (it runs as soon as resources are freed)
  - priority: the job is behind a higher priority job
*** print REASON part of a job
https://slurm.schedmd.com/squeue.html
squeue -u dhnam --format "%R"

** Check job information
- Job ID
  #+begin_src sh
  echo $SLURM_JOBID
  #+end_src
- Job Info
  #+begin_src sh
  scontrol show job [job_id]
  #+end_src
- Node Info
  #+begin_src sh
  scontrol show node [running_node_name]
  #+end_src
*** Chek remaining GPUs
https://stackoverflow.com/a/69691547
#+begin_src sh
scontrol show node [running_node_name]
#+end_src

- 'CfgTRES' says the configured GPUs
  e.g. the node has 8 GPUs
  #+begin_example
  CfgTRES=cpu=20,mem=250000M,billing=48,gres/gpu=8
  #+end_example

- AllocTRES says the allocated GPUs
  e.g. the node has 2 GPUs allocated
  #+begin_example
  AllocTRES=cpu=1,gres/gpu=2
  #+end_example

  e.g. no resource (including GPU) is used
  #+begin_example
  AllocTRES=
  #+end_example

- The number of available GPUs in a node is
  "num-of-configured-GPUs - num-of-allocated-GPUs"
  e.g. 8 - 2 = 6 GPUs are available

** Run
- interactive: run shell with resources
- batch: run batch script with resources
*** Interactive run
[srun/salloc options]
- p : partition 지정
- N : 총 할당받을 노드개수
- n, --ntasks : 실행할 task 개수
- t, -time : 최대 실행시간
--gres=gpu : 노드별 gpu 개수
--cpus-per-task 
**** srun only
2 nodes, 2 gpu per node

#+begin_src sh
srun -p titanxp -N 2 -n 4  -t 00:30:00 --gres=gpu:2 --pty /bin/bash -l  # resource allocation
exit  # exit from queue
#+end_src
***** examples
- 1 cpu
  #+begin_src sh
  srun -N 1 -n 1 -t 3-00:00:00 --pty /bin/bash -l
  srun -p [partition] -N 1 -n 1  -t 3-00:00:00 --pty /bin/bash -l
  #+end_src
- 2 cpus
  #+begin_src sh
  srun -N 1 -n 1 --cpus-per-task=2 -t 3-00:00:00 --pty /bin/bash -l
  #+end_src
**** salloc & srun
1) Allocate resources with salloc
2) Run tasks with srun

#+begin_src sh
salloc -N 1 -n 6  -t 00:30:00  /bin/bash -l  # resource allocation
srun execution.sh  # run task with the resources
exit  # exit from queue
#+end_src

*** Batch run
1) Write script with SBATCH options
2) run the script with sbatch commands
   #+begin_src sh
   sbatch tf-2gpu.slurm.sh
   #+end_src

[Example script]
#+begin_src sh
#!/bin/sh

#SBATCH -J  multi-2gpu   	# Job name
#SBATCH -o  multi-2gpu.%j.out    # Name of stdout output file (%j expands to %jobId)
#SBATCH -p gpu-titanxp     	# queue  name  or  partiton name gpu-titanxp,gpu-2080ti
#SBATCH -t 01:30:00 		# Run time (hh:mm:ss) - 1.5 hours

#### Select  GPU

## gpu 2장
#SBATCH   --gres=gpu:2

## gpu 4장
##SBATCH   --gres=gpu:4

## 노드 지정
##SBATCH  --nodelist=n1

## 노드 지정하지않기
#SBATCH   --nodes=1

## gpu 가 2장이면  --ntasks=2, --tasks-per-node=2 , --cpus-per-task=1
## gpu 가 4장이면  --ntasks=4, --tasks-per-node=4 , --cpus-per-task=1

#SBTACH   --ntasks=2
#SBATCH   --tasks-per-node=2
#SBATCH   --cpus-per-task=1

cd  $SLURM_SUBMIT_DIR

echo "SLURM_SUBMIT_DIR=$SLURM_SUBMIT_DIR"
echo "CUDA_HOME=$CUDA_HOME"
echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"
echo "CUDA_VERSION=$CUDA_VERSION"


srun -l /bin/hostname
srun -l /bin/pwd
srun -l /bin/date

module  purge
module  load   postech


SAMPLES_DIR=$HOME/TensorFlow-2.x-Tutorials/
python3  $SAMPLES_DIR/03-Play-with-MNIST/main.py

date

squeue  --job  $SLURM_JOBID

echo  "##### END #####"
#+end_src

** Stop jobs
- stop all my jobs
  #+begin_src sh
  scancel -u [user_id]
  #+end_src
- stop a specific job
  #+begin_src sh
  scancel [job_id]
  #+end_src
